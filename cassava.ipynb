{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cassava Leaf Disease Classification\n",
    "\n",
    "This notebook builds and trains a model for cassava leaf disease classification for the [Kaggle competition](https://www.kaggle.com/c/cassava-leaf-disease-classification/overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas\n",
    "\n",
    "1. Cross entropy loss, stratified CV, no fmix, cutmix, mixup, w gradient scaling & accumulation [done]\n",
    "2. add hyperparam tuning with raytune [done]\n",
    "2. Add smoothed cross entropy loss\n",
    "3. Add *mixes\n",
    "4. external data\n",
    "5. emsemble of models - train a model for each fold and then average their predictions during inference [done]\n",
    "6. train 15-20 epochs [done]\n",
    "7. Test time augmentation\n",
    "8. Better ensemble prediction - majority vote, other...?\n",
    "10. train a resnet model\n",
    "11. balanced classes instead of stratified?\n",
    "12. verify per class accuracy\n",
    "13. AdaBound - \"as good as SGD and as fast as Adam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import joblib\n",
    "import warnings\n",
    "import gc\n",
    "import errno\n",
    "import shutil\n",
    "\n",
    "# My modules\n",
    "from config import Config\n",
    "from logger import init_logger\n",
    "from common_utils import (set_seeds, read_csvs, stratify_split, setup_model_optimizer, \n",
    "                          get_data_dfs, get_loaders, create_holdout_loader, get_schd_crit)\n",
    "from model import Model\n",
    "from train_loop_functions import train_epoch, valid_epoch, ensemble_inference\n",
    "from cassava_dataset import CassavaDataset\n",
    "from early_stopping import EarlyStopping\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cassava Bacterial Blight (CBB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cassava Brown Streak Disease (CBSD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cassava Green Mottle (CGM)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cassava Mosaic Disease (CMD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     0\n",
       "0       Cassava Bacterial Blight (CBB)\n",
       "1  Cassava Brown Streak Disease (CBSD)\n",
       "2           Cassava Green Mottle (CGM)\n",
       "3         Cassava Mosaic Disease (CMD)\n",
       "4                              Healthy"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = pd.read_json(Config.data_dir + '/label_num_to_disease_map.json', orient='index')\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(Config.seed)\n",
    "LOGGER = init_logger() # uses Python's logging framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Functions\n",
    "\n",
    "gradient scaling https://pytorch.org/docs/stable/notes/amp_examples.html\n",
    "\n",
    "gradient accumulation https://towardsdatascience.com/what-is-gradient-accumulation-in-deep-learning-ec034122cfa\n",
    "\n",
    "https://towardsdatascience.com/deep-learning-model-training-loop-e41055a24b73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Trains the model over N epochs for a given fold\n",
    "    \n",
    "    train_folds_df: the dataset with a column for fold number\n",
    "    fold: an integer representing the fold used for validation\n",
    "    \n",
    "    Returns a DataFrame consisting of only the the rows used for validation along with the model's predictions\n",
    "''' \n",
    "def train_valid_test(train_folds_df, fold, resultsStore, device, \n",
    "                     experiment_name_dir, holdout_dataloader, holdout_targets, tb_writer):\n",
    "    \n",
    "    # -------- DATASETS AND LOADERS --------\n",
    "    # select one of the folds, create train & validation set loaders\n",
    "    train_df, valid_df = get_data_dfs(train_folds_df, fold)\n",
    "    train_dataloader, valid_dataloader = get_loaders(train_df, valid_df,\n",
    "                                                     Config.train_bs, \n",
    "                                                     Config.data_dir+'/train_images')\n",
    "    \n",
    "    \n",
    "    # make model and optimizer\n",
    "    model, optimizer = setup_model_optimizer(Config.model_arch, \n",
    "                                           Config.lr, \n",
    "                                           Config.is_amsgrad, \n",
    "                                           num_labels=train_folds_df.label.nunique(), \n",
    "                                           weight_decay=Config.weight_decay,\n",
    "                                           fc_layer={\"middle_fc\": False, \"middle_fc_size\": 0},\n",
    "                                           device=device,\n",
    "                                           checkpoint=None)\n",
    "\n",
    "    scheduler, criterion = get_schd_crit(optimizer)\n",
    "    \n",
    "    accuracy = 0.\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses, val_losses = [], []\n",
    "    early_stop = EarlyStopping('val_loss', LOGGER)\n",
    "\n",
    "    for e in range(Config.epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        LOGGER.info(f'Training epoch {e+1}/{Config.epochs}')\n",
    "        \n",
    "        # -------- TRAIN --------\n",
    "        avg_training_loss = train_epoch(train_dataloader, model, \n",
    "                                      criterion, optimizer, \n",
    "                                      scheduler, GradScaler(), \n",
    "                                      Config.accum_iter, LOGGER,\n",
    "                                      device, tb_writer, fold, e)\n",
    "\n",
    "        # -------- VALIDATE --------\n",
    "        avg_validation_loss, preds = valid_epoch(valid_dataloader, model, \n",
    "                                                 criterion, LOGGER, device, \n",
    "                                                 tb_writer, fold, e)\n",
    "        \n",
    "        train_losses.append(avg_training_loss)\n",
    "        val_losses.append(avg_validation_loss)\n",
    "\n",
    "        # -------- SCORE METRICS & LOGGING FOR THIS EPOCH --------\n",
    "        validation_labels = valid_df[Config.target_col].values\n",
    "        accuracy = accuracy_score(y_true=validation_labels, y_pred=preds)\n",
    "       \n",
    "        epoch_elapsed_time = time.time() - epoch_start_time\n",
    "        tb_writer.add_scalar(f'Avg Epoch Train Loss Fold {fold}', avg_training_loss, e)\n",
    "        tb_writer.add_scalar(f'Avg Epoch Val Loss Fold {fold}', avg_validation_loss, e)\n",
    "        tb_writer.add_scalar(f'Epoch Val Accuracy Fold {fold}', accuracy, e)\n",
    "        \n",
    "        LOGGER.info(f'\\nEpoch training summary:\\n Fold {fold+1}/{Config.fold_num} | ' + \\\n",
    "                    f'Epoch: {e+1}/{Config.epochs} | ' + \\\n",
    "                    f'Epoch time: {epoch_elapsed_time} sec\\n' + \\\n",
    "                    f'Training loss: {avg_training_loss} | ' + \\\n",
    "                    f'Validation loss: {avg_validation_loss} | ' + \\\n",
    "                    f'Accuracy: {accuracy}')\n",
    "        \n",
    "        early_stop(avg_validation_loss)\n",
    "        if early_stop.stop: break\n",
    "            \n",
    "        # --------SAVE MODEL --------\n",
    "        if avg_validation_loss < best_val_loss: \n",
    "            best_val_loss = avg_validation_loss\n",
    "            torch.save({'model': model.state_dict(), \n",
    "                        'accuracy': accuracy, \n",
    "                        'preds': preds,\n",
    "                        'val_loss': best_val_loss,\n",
    "                        'fold': fold\n",
    "                       },\n",
    "                      Config.save_dir + f'/{experiment_name_dir}/{Config.model_arch}_fold{fold}.pth')\n",
    "            LOGGER.info(f'Saved model!')\n",
    "        LOGGER.info('----------------')\n",
    "        # -------- UPDATE LR --------\n",
    "        if scheduler and e > 4:\n",
    "            if Config.scheduler == 'ReduceLROnPlateau':\n",
    "                scheduler.step(avg_validation_loss)\n",
    "            elif Config.scheduler == 'CosineAnnealingLR' or Config.scheduler == 'CosineAnnealingWarmRestarts':\n",
    "                scheduler.step()\n",
    "        gc.collect()\n",
    "\n",
    "    # -------- TEST ON HOLDOUT SET --------\n",
    "    # load best model\n",
    "    checkpoint = torch.load(Config.save_dir + f'/{experiment_name_dir}/{Config.model_arch}_fold{fold}.pth')\n",
    "    model.load_state_dict(checkpoint['model']) \n",
    "    # test\n",
    "    _, holdout_preds = valid_epoch(holdout_dataloader, model, criterion, LOGGER, device, tb_writer, holdout=True)\n",
    "    holdout_accuracy = accuracy_score(y_true=holdout_targets, y_pred=holdout_preds)\n",
    "    \n",
    "    valid_df['prediction'] = checkpoint['preds']\n",
    "    del model\n",
    "    del optimizer\n",
    "    del train_dataloader\n",
    "    del valid_dataloader\n",
    "    return valid_df, checkpoint['accuracy'], holdout_accuracy, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Results:\n",
    "    def __init__(self):\n",
    "        self.fold_to_predictions = []\n",
    "        self.fold_to_accuracy = []\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "\"\"\"\n",
    "Entry point to training and inference. \n",
    "experiment_name_dir (required): a name for a directory in ./trained-models \n",
    "kaggle (required): indicates whether to run on kaggle test set\n",
    "\"\"\"\n",
    "def main(experiment_name_dir, kaggle):\n",
    "    base_experiment_filename = Config.save_dir + f'/{experiment_name_dir}/{Config.model_arch}_fold'\n",
    "    try:\n",
    "        # -------- SETUP --------\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        resultsStore = Results()\n",
    "        tb_writer = SummaryWriter(f'./runs/{experiment_name_dir}')\n",
    "        \n",
    "        # -------- LOAD DATA FROM FILE --------\n",
    "        data_df, sample_df, holdout_df = read_csvs(Config.data_dir, Config.debug, test_proportion=0.15)\n",
    "        folds = stratify_split(data_df, Config.fold_num, Config.seed, Config.target_col)\n",
    "        test_df, test_loader = None, None\n",
    "        \n",
    "        # create holdout dataloader to test on totally unseen data\n",
    "        holdout_dataloader, holdout_targets = create_holdout_loader(holdout_df, Config.data_dir + '/train_images')   \n",
    "\n",
    "        experiment_list = os.listdir(Config.save_dir)\n",
    "        if not Config.inference and experiment_name_dir in experiment_list: # resume training from the last fold's checkpoint\n",
    "            last_fold = len(os.listdir(Config.save_dir + f'/{experiment_name_dir}')) - 1\n",
    "            if last_fold >= 0: \n",
    "                print(f'Experiment exists. Resuming training from latest fold ({last_fold}).')\n",
    "\n",
    "                checkpoint = torch.load(base_experiment_filename + f'{last_fold}.pth')\n",
    "\n",
    "                #resume(checkpoint, fold, model, optimizer)\n",
    "        else: # -------- START TRAINING --------\n",
    "            if Config.train:\n",
    "                # make directory for experiment\n",
    "                try:\n",
    "                    os.makedirs(Config.save_dir + f'/{experiment_name_dir}')\n",
    "                    # copy the config file to this directory\n",
    "                    shutil.copy2('./config.py', Config.save_dir + f'/{experiment_name_dir}')\n",
    "                except OSError as e:\n",
    "                    if e.errno != errno.EEXIST:\n",
    "                        raise\n",
    "                LOGGER.info('\\n========== Running training ==========\\n')\n",
    "\n",
    "                aggregated_output_df = pd.DataFrame()\n",
    "                \n",
    "                time_training_start = time.time()\n",
    "                for fold in range(Config.fold_num):    \n",
    "                    # _df is the validation prediction output\n",
    "                    # _df.columns: ['image_id', 'label', 'fold', 'prediction']\n",
    "                    _df, val_accuracy, holdout_accuracy, train_losses, val_losses = train_valid_test(\n",
    "                                                                        folds, fold, \n",
    "                                                                        resultsStore, device,\n",
    "                                                                        experiment_name_dir,\n",
    "                                                                        holdout_dataloader, \n",
    "                                                                        holdout_targets, tb_writer)\n",
    "                    resultsStore.train_losses.append(train_losses)\n",
    "                    resultsStore.val_losses.append(val_losses)\n",
    "                    \n",
    "                    if aggregated_output_df.empty:\n",
    "                        aggregated_output_df[['image_id', 'label']] = _df[['image_id', 'label']]\n",
    "                    aggregated_output_df[['prediction_fold'+str(fold)]] = _df['prediction']\n",
    "\n",
    "                    resultsStore.fold_to_predictions.append(_df[['image_id', 'label', 'prediction']])\n",
    "                    resultsStore.fold_to_accuracy.append((val_accuracy, holdout_accuracy))\n",
    "\n",
    "                    LOGGER.info(f'========== fold: {fold+1}/{Config.fold_num} result ==========')\n",
    "                    LOGGER.info(f'Validation Accuracy: {val_accuracy}')\n",
    "                    LOGGER.info(f'Holdout Accuracy: {holdout_accuracy}')\n",
    "\n",
    "                # Cross validation\n",
    "                time_elapsed_training = time.time() - time_training_start \n",
    "                LOGGER.info(f\"Training time: {str(timedelta(seconds=time_elapsed_training))}\")\n",
    "                LOGGER.info(f\"========== CV ==========\") # best results across all folds\n",
    "                LOGGER.info(f\"{resultsStore.fold_to_accuracy}\")\n",
    "\n",
    "                # Save result\n",
    "                aggregated_output_df.to_csv(Config.save_dir + f'/{experiment_name_dir}/aggregated_output_df.csv', index=False)                \n",
    "                \n",
    "        if Config.inference: # runs inference on all trained models, averages result\n",
    "            LOGGER.info('\\n========== Running inference ==========\\n')\n",
    "            \n",
    "            model_states = [torch.load(base_experiment_filename + f'{fold}.pth')['model']\n",
    "                            for fold in range(Config.fold_num)]\n",
    "            assert len(model_states) == Config.fold_num\n",
    "            \n",
    "            \n",
    "            if not kaggle: \n",
    "                loader = holdout_dataloader\n",
    "                num_samples = len(holdout_df)\n",
    "            else: \n",
    "                loader = test_dataloader \n",
    "                num_samples = len(test_df)\n",
    "            \n",
    "            inference_start = time.time()\n",
    "            \n",
    "            predictions = ensemble_inference(model_states, Config.model_arch, \n",
    "                                    data_df.label.nunique(), loader, num_samples, device)\n",
    "            \n",
    "            inference_elapsed = time.time() - inference_start\n",
    "            LOGGER.info(f\"Inference time: {str(timedelta(seconds=inference_elapsed))}\")\n",
    "            \n",
    "            if not kaggle:\n",
    "                holdout_accuracy = accuracy_score(y_true=holdout_targets, y_pred=predictions)\n",
    "                LOGGER.info(f\"Ensemble model holdout accuracy: {holdout_accuracy}\")\n",
    "            \n",
    "            # submission\n",
    "            if kaggle:\n",
    "                submission = pd.DataFrame()\n",
    "                submission['image_id'] = test_df['image_id']\n",
    "                submission['label'] = predictions\n",
    "                submission.to_csv('submission.csv', index=False)\n",
    "        return resultsStore\n",
    "    finally: \n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsStore = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Running training ==========\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in debug mode: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1/20\n",
      "  0%|          | 0/853 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0: adjusting learning rate of group 0 to 1.2185e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TRAIN] Processing batch 8:   1%|          | 7/853 [00:07<12:54,  1.09it/s]/opt/favordata/AI/Felix/kaggle-cassava/adabound.py:94: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370128159/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "[TRAIN] Processing batch 853: 100%|██████████| 853/853 [07:41<00:00,  1.85it/s]\n",
      "\n",
      "[TRAIN] batch loss: 0.07326995573309004\n",
      "[VAL] Processing batch 143: 100%|██████████| 143/143 [01:38<00:00,  1.46it/s]\n",
      "\n",
      "[VAL] batch loss: 0.36422240853934856\n",
      "\n",
      "Epoch training summary:\n",
      " Fold 1/4 | Epoch: 1/20 | Epoch time: 559.6121728420258 sec\n",
      "Training loss: 0.07326995573309004 | Validation loss: 0.36422240853934856 | Accuracy: 0.8759621728612271\n",
      "\n",
      "Saved model!\n",
      "Training epoch 2/20\n",
      "[TRAIN] Processing batch 853: 100%|██████████| 853/853 [07:38<00:00,  1.86it/s]\n",
      "\n",
      "[TRAIN] batch loss: 0.05338739637713465\n",
      "[VAL] Processing batch 143: 100%|██████████| 143/143 [01:39<00:00,  1.44it/s]\n",
      "\n",
      "[VAL] batch loss: 0.34871530835028297\n",
      "\n",
      "Epoch training summary:\n",
      " Fold 1/4 | Epoch: 2/20 | Epoch time: 557.7124872207642 sec\n",
      "Training loss: 0.05338739637713465 | Validation loss: 0.34871530835028297 | Accuracy: 0.8825599296239278\n",
      "\n",
      "Saved model!\n",
      "Training epoch 3/20\n",
      "[TRAIN] Processing batch 853: 100%|██████████| 853/853 [07:37<00:00,  1.86it/s]\n",
      "\n",
      "[TRAIN] batch loss: 0.048828345190096045\n",
      "[VAL] Processing batch 143: 100%|██████████| 143/143 [01:36<00:00,  1.48it/s]\n",
      "\n",
      "[VAL] batch loss: 0.3396779043482734\n",
      "\n",
      "Epoch training summary:\n",
      " Fold 1/4 | Epoch: 3/20 | Epoch time: 554.6041488647461 sec\n",
      "Training loss: 0.048828345190096045 | Validation loss: 0.3396779043482734 | Accuracy: 0.8865185836815482\n",
      "\n",
      "Saved model!\n",
      "Training epoch 4/20\n",
      "[TRAIN] Processing batch 853: 100%|██████████| 853/853 [07:30<00:00,  1.90it/s]\n",
      "\n",
      "[TRAIN] batch loss: 0.04642347332553071\n",
      "[VAL] Processing batch 143: 100%|██████████| 143/143 [01:36<00:00,  1.48it/s]\n",
      "\n",
      "[VAL] batch loss: 0.3312200978137813\n",
      "\n",
      "Epoch training summary:\n",
      " Fold 1/4 | Epoch: 4/20 | Epoch time: 546.799721956253 sec\n",
      "Training loss: 0.04642347332553071 | Validation loss: 0.3312200978137813 | Accuracy: 0.8873982845832417\n",
      "\n",
      "Saved model!\n",
      "Training epoch 5/20\n",
      "[TRAIN] Processing batch 853: 100%|██████████| 853/853 [07:38<00:00,  1.86it/s]\n",
      "\n",
      "[TRAIN] batch loss: 0.04361632272654627\n",
      "[VAL] Processing batch 143: 100%|██████████| 143/143 [01:38<00:00,  1.45it/s]\n",
      "\n",
      "[VAL] batch loss: 0.3367429583274818\n",
      "\n",
      "Epoch training summary:\n",
      " Fold 1/4 | Epoch: 5/20 | Epoch time: 557.4066042900085 sec\n",
      "Training loss: 0.04361632272654627 | Validation loss: 0.3367429583274818 | Accuracy: 0.8871783593578183\n",
      "\n",
      "Training epoch 6/20\n",
      "[TRAIN] Processing batch 853: 100%|██████████| 853/853 [07:39<00:00,  1.86it/s]\n",
      "\n",
      "[TRAIN] batch loss: 0.0408001636626616\n",
      "[VAL] Processing batch 143: 100%|██████████| 143/143 [01:35<00:00,  1.50it/s]\n",
      "\n",
      "[VAL] batch loss: 0.33695057420047014\n",
      "\n",
      "Epoch training summary:\n",
      " Fold 1/4 | Epoch: 6/20 | Epoch time: 554.5415296554565 sec\n",
      "Training loss: 0.0408001636626616 | Validation loss: 0.33695057420047014 | Accuracy: 0.8873982845832417\n",
      "\n",
      "Training epoch 7/20\n",
      "  0%|          | 0/853 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     1: adjusting learning rate of group 0 to 1.1911e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TRAIN] Processing batch 853: 100%|██████████| 853/853 [07:38<00:00,  1.86it/s]\n",
      "\n",
      "[TRAIN] batch loss: 0.04031860592948646\n",
      "[VAL] Processing batch 143: 100%|██████████| 143/143 [01:37<00:00,  1.47it/s]\n",
      "\n",
      "[VAL] batch loss: 0.33997241988569704\n",
      "\n",
      "Epoch training summary:\n",
      " Fold 1/4 | Epoch: 7/20 | Epoch time: 555.9259831905365 sec\n",
      "Training loss: 0.04031860592948646 | Validation loss: 0.33997241988569704 | Accuracy: 0.887618209808665\n",
      "\n",
      "Training epoch 8/20\n",
      "  0%|          | 0/853 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     2: adjusting learning rate of group 0 to 1.1117e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TRAIN] Processing batch 853: 100%|██████████| 853/853 [07:39<00:00,  1.86it/s]\n",
      "\n",
      "[TRAIN] batch loss: 0.038369858219285014\n",
      "[VAL] Processing batch 143: 100%|██████████| 143/143 [01:36<00:00,  1.48it/s]\n",
      "\n",
      "[VAL] batch loss: 0.3284887628486523\n",
      "\n",
      "Epoch training summary:\n",
      " Fold 1/4 | Epoch: 8/20 | Epoch time: 555.8765950202942 sec\n",
      "Training loss: 0.038369858219285014 | Validation loss: 0.3284887628486523 | Accuracy: 0.8895975368374752\n",
      "\n",
      "Saved model!\n",
      "Training epoch 9/20\n",
      "  0%|          | 0/853 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     3: adjusting learning rate of group 0 to 9.8797e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TRAIN] Processing batch 853: 100%|██████████| 853/853 [07:29<00:00,  1.90it/s]\n",
      "\n",
      "[TRAIN] batch loss: 0.03704073981426717\n",
      "[VAL] Processing batch 143: 100%|██████████| 143/143 [01:37<00:00,  1.47it/s]\n",
      "\n",
      "[VAL] batch loss: 0.34678934359519215\n",
      "\n",
      "Epoch training summary:\n",
      " Fold 1/4 | Epoch: 9/20 | Epoch time: 546.9207141399384 sec\n",
      "Training loss: 0.03704073981426717 | Validation loss: 0.34678934359519215 | Accuracy: 0.8904772377391686\n",
      "\n",
      "Training epoch 10/20\n",
      "  0%|          | 0/853 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     4: adjusting learning rate of group 0 to 8.3207e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TRAIN] Processing batch 853: 100%|██████████| 853/853 [07:38<00:00,  1.86it/s]\n",
      "\n",
      "[TRAIN] batch loss: 0.03663248365257996\n",
      "[VAL] Processing batch 143: 100%|██████████| 143/143 [01:36<00:00,  1.48it/s]\n",
      "\n",
      "[VAL] batch loss: 0.34234492366130537\n",
      "\n",
      "Epoch training summary:\n",
      " Fold 1/4 | Epoch: 10/20 | Epoch time: 555.8240222930908 sec\n",
      "Training loss: 0.03663248365257996 | Validation loss: 0.34234492366130537 | Accuracy: 0.8935561908950956\n",
      "\n",
      "Training epoch 11/20\n",
      "  0%|          | 0/853 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     5: adjusting learning rate of group 0 to 6.5925e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TRAIN] Processing batch 853: 100%|██████████| 853/853 [07:38<00:00,  1.86it/s]\n",
      "\n",
      "[TRAIN] batch loss: 0.034545274017376504\n",
      "[VAL] Processing batch 143: 100%|██████████| 143/143 [01:37<00:00,  1.47it/s]\n",
      "\n",
      "[VAL] batch loss: 0.34237468406766447\n",
      "\n",
      "Epoch training summary:\n",
      " Fold 1/4 | Epoch: 11/20 | Epoch time: 556.1295082569122 sec\n",
      "Training loss: 0.034545274017376504 | Validation loss: 0.34237468406766447 | Accuracy: 0.8931163404442489\n",
      "\n",
      "Training epoch 12/20\n",
      "  0%|          | 0/853 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     6: adjusting learning rate of group 0 to 4.8643e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TRAIN] Processing batch 853: 100%|██████████| 853/853 [07:39<00:00,  1.86it/s]\n",
      "\n",
      "[TRAIN] batch loss: 0.03389213820807483\n",
      "[VAL] Processing batch 143: 100%|██████████| 143/143 [01:38<00:00,  1.45it/s]\n",
      "\n",
      "[VAL] batch loss: 0.3406490725497683\n",
      "\n",
      "Epoch training summary:\n",
      " Fold 1/4 | Epoch: 12/20 | Epoch time: 558.1848247051239 sec\n",
      "Training loss: 0.03389213820807483 | Validation loss: 0.3406490725497683 | Accuracy: 0.8913569386408621\n",
      "\n",
      "Metric val_loss has not seen improvement in 4 epochs. Early stop.\n",
      "[VAL] Processing batch 101: 100%|██████████| 101/101 [01:10<00:00,  1.43it/s]\n",
      "\n",
      "[VAL] batch loss: 0.34457306497462903\n",
      "========== fold: 1/4 result ==========\n",
      "Validation Accuracy: 0.8895975368374752\n",
      "Holdout Accuracy: 0.8800623052959502\n",
      "Training epoch 1/20\n",
      "  0%|          | 0/853 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0: adjusting learning rate of group 0 to 1.2185e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TRAIN] Processing batch 853: 100%|██████████| 853/853 [07:30<00:00,  1.89it/s]\n",
      "\n",
      "[TRAIN] batch loss: 0.06988417021144107\n",
      "[VAL] Processing batch 143: 100%|██████████| 143/143 [01:35<00:00,  1.50it/s]\n",
      "\n",
      "[VAL] batch loss: 0.40336341572391404\n",
      "\n",
      "Epoch training summary:\n",
      " Fold 2/4 | Epoch: 1/20 | Epoch time: 545.9302754402161 sec\n",
      "Training loss: 0.06988417021144107 | Validation loss: 0.40336341572391404 | Accuracy: 0.8660655377171762\n",
      "\n",
      "Saved model!\n",
      "Training epoch 2/20\n",
      "[TRAIN] Processing batch 297:  35%|███▍      | 297/853 [02:35<04:50,  1.91it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        print('Running in debug mode:', Config.debug)\n",
    "        resultsStore = main(experiment_name_dir='exp3_adabound', kaggle=False)\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
