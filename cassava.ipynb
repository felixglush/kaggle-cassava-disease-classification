{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cassava Leaf Disease Classification\n",
    "\n",
    "This notebook builds and trains a model for cassava leaf disease classification for the [Kaggle competition](https://www.kaggle.com/c/cassava-leaf-disease-classification/overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas\n",
    "\n",
    "1. Cross entropy loss, stratified CV, no fmix, cutmix, mixup, w gradient scaling & accumulation [done]\n",
    "2. add hyperparam tuning with raytune [done]\n",
    "2. Add smoothed cross entropy loss\n",
    "3. Add *mixes\n",
    "4. external data\n",
    "5. emsemble of models - train a model for each fold and then average their predictions during inference [done]\n",
    "6. train 15-20 epochs\n",
    "7. Test time augmentation\n",
    "8. Better ensemble prediction - majority vote, other...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import joblib\n",
    "import warnings\n",
    "import gc\n",
    "import errno\n",
    "import shutil\n",
    "\n",
    "# My modules\n",
    "from config import Config\n",
    "from logger import init_logger\n",
    "from common_utils import (set_seeds, read_csvs, stratify_split, setup_model_optimizer, \n",
    "                          get_data_dfs, get_loaders, create_holdout_loader, get_schd_crit)\n",
    "from model import Model\n",
    "from train_loop_functions import train_epoch, valid_epoch, ensemble_inference\n",
    "from cassava_dataset import CassavaDataset\n",
    "from early_stopping import EarlyStopping\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.cuda.amp import GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cassava Bacterial Blight (CBB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cassava Brown Streak Disease (CBSD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cassava Green Mottle (CGM)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cassava Mosaic Disease (CMD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     0\n",
       "0       Cassava Bacterial Blight (CBB)\n",
       "1  Cassava Brown Streak Disease (CBSD)\n",
       "2           Cassava Green Mottle (CGM)\n",
       "3         Cassava Mosaic Disease (CMD)\n",
       "4                              Healthy"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = pd.read_json(Config.data_dir + '/label_num_to_disease_map.json', orient='index')\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(Config.seed)\n",
    "LOGGER = init_logger() # uses Python's logging framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Functions\n",
    "\n",
    "gradient scaling https://pytorch.org/docs/stable/notes/amp_examples.html\n",
    "\n",
    "gradient accumulation https://towardsdatascience.com/what-is-gradient-accumulation-in-deep-learning-ec034122cfa\n",
    "\n",
    "https://towardsdatascience.com/deep-learning-model-training-loop-e41055a24b73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Trains the model over N epochs for a given fold\n",
    "    \n",
    "    train_folds_df: the dataset with a column for fold number\n",
    "    fold: an integer representing the fold used for validation\n",
    "    \n",
    "    Returns a DataFrame consisting of only the the rows used for validation along with the model's predictions\n",
    "''' \n",
    "def train_valid_test(train_folds_df, fold, resultsStore, device, \n",
    "                     experiment_name_dir, holdout_dataloader, holdout_targets):\n",
    "    \n",
    "    # -------- DATASETS AND LOADERS --------\n",
    "    # select one of the folds, create train & validation set loaders\n",
    "    train_df, valid_df = get_data_dfs(train_folds_df, fold)\n",
    "    train_dataloader, valid_dataloader = get_loaders(train_df, valid_df,\n",
    "                                                     Config.train_bs, \n",
    "                                                     Config.data_dir+'/train_images')\n",
    "    \n",
    "    \n",
    "    # make model and optimizer\n",
    "    model, optimizer = setup_model_optimizer(Config.model_arch, \n",
    "                                           Config.lr, \n",
    "                                           Config.is_amsgrad, \n",
    "                                           num_labels=train_folds_df.label.nunique(), \n",
    "                                           weight_decay=Config.weight_decay,\n",
    "                                           fc_layer={\"middle_fc\": False, \"middle_fc_size\": 0},\n",
    "                                           device=device,\n",
    "                                           checkpoint=None)\n",
    "\n",
    "    scheduler, criterion = get_schd_crit(optimizer)\n",
    "    \n",
    "    accuracy = 0.\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses, val_losses = [], []\n",
    "    early_stop = EarlyStopping('val_loss', LOGGER)\n",
    "    \n",
    "    for e in range(Config.epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        LOGGER.info(f'Training epoch {e+1}/{Config.epochs}')\n",
    "        \n",
    "        # -------- TRAIN --------\n",
    "        avg_training_loss = train_epoch(train_dataloader, model, \n",
    "                                      criterion, optimizer, \n",
    "                                      scheduler, GradScaler(), \n",
    "                                      Config.accum_iter, LOGGER,\n",
    "                                      device)\n",
    "\n",
    "        # -------- VALIDATE --------\n",
    "        avg_validation_loss, preds = valid_epoch(valid_dataloader, model, \n",
    "                                                 criterion, LOGGER, device)\n",
    "        \n",
    "        train_losses.append(avg_training_loss)\n",
    "        val_losses.append(avg_validation_loss)\n",
    "\n",
    "        # -------- SCORE METRICS & LOGGING FOR THIS EPOCH --------\n",
    "        validation_labels = valid_df[Config.target_col].values\n",
    "        accuracy = accuracy_score(y_true=validation_labels, y_pred=preds)\n",
    "       \n",
    "        epoch_elapsed_time = time.time() - epoch_start_time\n",
    "        \n",
    "        LOGGER.info(f'\\nEpoch training summary:\\n Fold {fold+1}/{Config.fold_num} | ' + \\\n",
    "                    f'Epoch: {e+1}/{Config.epochs} | ' + \\\n",
    "                    f'Epoch time: {epoch_elapsed_time} sec\\n' + \\\n",
    "                    f'Training loss: {avg_training_loss} | ' + \\\n",
    "                    f'Validation loss: {avg_validation_loss} | ' + \\\n",
    "                    f'Accuracy: {accuracy}\\n')\n",
    "        \n",
    "        early_stop(avg_validation_loss)\n",
    "        if early_stop.stop: break\n",
    "            \n",
    "        # --------SAVE MODEL --------\n",
    "        if avg_validation_loss < best_val_loss: \n",
    "            best_val_loss = avg_validation_loss\n",
    "            torch.save({'model': model.state_dict(), \n",
    "                        'accuracy': accuracy, \n",
    "                        'preds': preds,\n",
    "                        'val_loss': best_val_loss,\n",
    "                        'fold': fold\n",
    "                       },\n",
    "                      Config.save_dir + f'/{experiment_name_dir}/{Config.model_arch}_fold{fold}.pth')\n",
    "            LOGGER.info(f'Saved model!')\n",
    "        \n",
    "        # -------- UPDATE LR --------\n",
    "        if scheduler and e > 2:\n",
    "            if Config.scheduler == 'ReduceLROnPlateau':\n",
    "                scheduler.step(avg_validation_loss)\n",
    "            elif Config.scheduler == 'CosineAnnealingLR' or Config.scheduler == 'CosineAnnealingWarmRestarts':\n",
    "                scheduler.step()\n",
    "        gc.collect()\n",
    "\n",
    "    # -------- TEST ON HOLDOUT SET --------\n",
    "    # load best model\n",
    "    checkpoint = torch.load(Config.save_dir + f'/{experiment_name_dir}/{Config.model_arch}_fold{fold}.pth')\n",
    "    model.load_state_dict(checkpoint['model']) \n",
    "    # test\n",
    "    _, holdout_preds = valid_epoch(holdout_dataloader, model, criterion, LOGGER, device)\n",
    "    holdout_accuracy = accuracy_score(y_true=holdout_targets, y_pred=holdout_preds)\n",
    "    \n",
    "    valid_df['prediction'] = checkpoint['preds']\n",
    "    del model\n",
    "    del optimizer\n",
    "    del train_dataloader\n",
    "    del valid_dataloader\n",
    "    return valid_df, checkpoint['accuracy'], holdout_accuracy, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Results:\n",
    "    def __init__(self):\n",
    "        self.fold_to_predictions = []\n",
    "        self.fold_to_accuracy = []\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "\"\"\"\n",
    "Entry point to training and inference. \n",
    "experiment_name_dir (required): a name for a directory in ./trained-models \n",
    "kaggle (required): indicates whether to run on kaggle test set\n",
    "\"\"\"\n",
    "def main(experiment_name_dir, kaggle):\n",
    "    base_experiment_filename = Config.save_dir + f'/{experiment_name_dir}/{Config.model_arch}_fold'\n",
    "    \n",
    "    try:\n",
    "        # -------- SETUP --------\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        resultsStore = Results()\n",
    "        \n",
    "\n",
    "        # -------- LOAD DATA FROM FILE --------\n",
    "        data_df, sample_df, holdout_df = read_csvs(Config.data_dir, Config.debug, test_proportion=0.15)\n",
    "        folds = stratify_split(data_df, Config.fold_num, Config.seed, Config.target_col)\n",
    "        test_df, test_loader = None, None\n",
    "        \n",
    "        # create holdout dataloader to test on totally unseen data\n",
    "        holdout_dataloader, holdout_targets = create_holdout_loader(holdout_df, Config.data_dir + '/train_images')   \n",
    "\n",
    "        experiment_list = os.listdir(Config.save_dir)\n",
    "        if not Config.inference and experiment_name_dir in experiment_list: # resume training from the last fold's checkpoint\n",
    "            last_fold = len(os.listdir(Config.save_dir + f'/{experiment_name_dir}')) - 1\n",
    "            if last_fold >= 0: \n",
    "                print(f'Experiment exists. Resuming training from latest fold ({last_fold}).')\n",
    "\n",
    "                checkpoint = torch.load(base_experiment_filename + f'{last_fold}.pth')\n",
    "\n",
    "                #resume(checkpoint, fold, model, optimizer)\n",
    "        else: # -------- START TRAINING --------\n",
    "            if Config.train:\n",
    "                # make directory for experiment\n",
    "                try:\n",
    "                    os.makedirs(Config.save_dir + f'/{experiment_name_dir}')\n",
    "                    # copy the config file to this directory\n",
    "                    shutil.copy2('./config.py', Config.save_dir + f'/{experiment_name_dir}')\n",
    "                except OSError as e:\n",
    "                    if e.errno != errno.EEXIST:\n",
    "                        raise\n",
    "                LOGGER.info('\\n========== Running training ==========\\n')\n",
    "\n",
    "                aggregated_output_df = pd.DataFrame()\n",
    "                \n",
    "                time_training_start = time.time()\n",
    "                for fold in range(Config.fold_num):    \n",
    "                    # _df is the validation prediction output\n",
    "                    # _df.columns: ['image_id', 'label', 'fold', 'prediction']\n",
    "                    _df, val_accuracy, holdout_accuracy, train_losses, val_losses = train_valid_test(\n",
    "                                                                        folds, fold, \n",
    "                                                                        resultsStore, device,\n",
    "                                                                        experiment_name_dir,\n",
    "                                                                        holdout_dataloader, \n",
    "                                                                        holdout_targets)\n",
    "                    resultsStore.train_losses.append(train_losses)\n",
    "                    resultsStore.val_losses.append(val_losses)\n",
    "                    \n",
    "                    if aggregated_output_df.empty:\n",
    "                        aggregated_output_df[['image_id', 'label']] = _df[['image_id', 'label']]\n",
    "                    aggregated_output_df[['prediction_fold'+str(fold)]] = _df['prediction']\n",
    "\n",
    "                    resultsStore.fold_to_predictions.append(_df[['image_id', 'label', 'prediction']])\n",
    "                    resultsStore.fold_to_accuracy.append((val_accuracy, holdout_accuracy))\n",
    "\n",
    "                    LOGGER.info(f'========== fold: {fold+1}/{Config.fold_num} result ==========')\n",
    "                    LOGGER.info(f'Validation Accuracy: {val_accuracy}')\n",
    "                    LOGGER.info(f'Holdout Accuracy: {holdout_accuracy}')\n",
    "\n",
    "                # Cross validation\n",
    "                time_elapsed_training = time.time() - time_training_start \n",
    "                LOGGER.info(f\"Training time: {str(timedelta(seconds=time_elapsed_training))}\")\n",
    "                LOGGER.info(f\"========== CV ==========\") # best results across all folds\n",
    "                LOGGER.info(f\"{resultsStore.fold_to_accuracy}\")\n",
    "\n",
    "                # Save result\n",
    "                aggregated_output_df.to_csv(Config.save_dir + f'/{experiment_name_dir}/aggregated_output_df.csv', index=False)\n",
    "    \n",
    "        if Config.inference: # runs inference on all trained models, averages result\n",
    "            LOGGER.info('\\n========== Running inference ==========\\n')\n",
    "            \n",
    "            model_states = [torch.load(base_experiment_filename + f'{fold}.pth')['model']\n",
    "                            for fold in range(Config.fold_num)]\n",
    "            assert len(model_states) == Config.fold_num\n",
    "            \n",
    "            \n",
    "            if not kaggle: \n",
    "                loader = holdout_dataloader\n",
    "                num_samples = len(holdout_df)\n",
    "            else: \n",
    "                loader = test_dataloader \n",
    "                num_samples = len(test_df)\n",
    "            \n",
    "            inference_start = time.time()\n",
    "            \n",
    "            predictions = ensemble_inference(model_states, Config.model_arch, \n",
    "                                    data_df.label.nunique(), loader, num_samples, device)\n",
    "            \n",
    "            inference_elapsed = time.time() - inference_start\n",
    "            LOGGER.info(f\"Inference time: {str(timedelta(seconds=inference_elapsed))}\")\n",
    "            \n",
    "            if not kaggle:\n",
    "                holdout_accuracy = accuracy_score(y_true=holdout_targets, y_pred=predictions)\n",
    "                LOGGER.info(f\"Ensemble model holdout accuracy: {holdout_accuracy}\")\n",
    "            \n",
    "            # submission\n",
    "            if kaggle:\n",
    "                submission = pd.DataFrame()\n",
    "                submission['image_id'] = test_df['image_id']\n",
    "                submission['label'] = predictions\n",
    "                submission.to_csv('submission.csv', index=False)\n",
    "        return resultsStore\n",
    "    finally: \n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsStore = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Running training ==========\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in debug mode: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0: adjusting learning rate of group 0 to 1.2185e-03.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        print('Running in debug mode:', Config.debug)\n",
    "        resultsStore = main(experiment_name_dir='exp1', kaggle=False)\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
