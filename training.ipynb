{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cassava Leaf Disease Classification\n",
    "\n",
    "This notebook builds and trains a model for cassava leaf disease classification for the [Kaggle competition](https://www.kaggle.com/c/cassava-leaf-disease-classification/overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas\n",
    "\n",
    "1. Cross entropy loss, stratified CV, no fmix, cutmix, mixup, w gradient scaling & accumulation [done]\n",
    "2. add hyperparam tuning with raytune [done]\n",
    "2. Add smoothed cross entropy loss\n",
    "3. Add *mixes\n",
    "4. external data\n",
    "5. emsemble of models - train a model for each fold and then average their predictions during inference [done]\n",
    "6. train 15-20 epochs [done]\n",
    "7. Test time augmentation\n",
    "8. Better ensemble prediction - majority vote [done], other...?\n",
    "10. train a resnet model\n",
    "11. balanced classes instead of stratified?\n",
    "12. verify per class accuracy\n",
    "13. AdaBound - \"as good as SGD and as fast as Adam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import joblib\n",
    "import warnings\n",
    "import gc\n",
    "import errno\n",
    "import shutil\n",
    "\n",
    "# My modules\n",
    "from config import Config\n",
    "from logger import init_logger\n",
    "from common_utils import (set_seeds, read_csvs, stratify_split, setup_model_optimizer, \n",
    "                          get_data_dfs, get_loaders, create_holdout_loader, get_schd_crit)\n",
    "from model import Model\n",
    "from train_loop_functions import train_epoch, valid_epoch\n",
    "from cassava_dataset import CassavaDataset\n",
    "from early_stopping import EarlyStopping\n",
    "from trainer import Trainer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = pd.read_json(Config.data_dir + '/label_num_to_disease_map.json', orient='index')\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(Config.seed)\n",
    "LOGGER = init_logger() # uses Python's logging framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "https://towardsdatascience.com/deep-learning-model-training-loop-e41055a24b73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-1639c8eba3c1>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Work in progress. # todo: implement an encapsulating class for the training loops and store all the state variables in it so we don't have to pass them as parameters everywhere\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mtrainer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTrainer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlogger\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtensorboard_writer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalid_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mholdout_loader\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'Trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# Work in progress: implement an encapsulating class for the training loops \n",
    "# and store all the state variables in it so we don't have to pass them as parameters everywhere\n",
    "\n",
    "trainer = Trainer(logger, tensorboard_writer, device, train_loader, valid_loader, holdout_loader)\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Trains the model over N epochs for a given fold\n",
    "    \n",
    "    train_folds_df: the dataset with a column for fold number\n",
    "    fold: an integer representing the fold used for validation\n",
    "    \n",
    "    Returns a DataFrame consisting of only the the rows used for validation along with the model's predictions\n",
    "''' \n",
    "def train_valid_test(train_folds_df, fold, \n",
    "                     device, basename, \n",
    "                     holdout_dataloader, holdout_targets, \n",
    "                     tb_writer, checkpoint=None):\n",
    "    model_checkpoint_name = basename + f'/{Config.model_arch}_fold{fold}.pth'\n",
    "    \n",
    "    # -------- DATASETS AND LOADERS --------\n",
    "    # select one of the folds, create train & validation set loaders\n",
    "    train_df, valid_df = get_data_dfs(train_folds_df, fold)\n",
    "    train_dataloader, valid_dataloader = get_loaders(train_df, valid_df,\n",
    "                                                     Config.train_bs, \n",
    "                                                     Config.data_dir + '/train_images')\n",
    "    \n",
    "    # make model and optimizer\n",
    "    model, optimizer = setup_model_optimizer(Config.model_arch, \n",
    "                                           Config.lr, \n",
    "                                           Config.is_amsgrad, \n",
    "                                           num_labels=train_folds_df.label.nunique(), \n",
    "                                           weight_decay=Config.weight_decay,\n",
    "                                           momentum=Config.momentum,\n",
    "                                           fc_layer={\"middle_fc\": False, \"middle_fc_size\": 0},\n",
    "                                           device=device,\n",
    "                                           checkpoint=checkpoint)\n",
    "    \n",
    "    scheduler, criterion = get_schd_crit(optimizer)\n",
    "    \n",
    "    accuracy = 0.\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    early_stop = EarlyStopping('val_loss', LOGGER, patience=Config.loss_patience)\n",
    "\n",
    "    for e in range(Config.epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        LOGGER.info(f'Training epoch {e+1}/{Config.epochs}')\n",
    "        \n",
    "        # -------- TRAIN --------\n",
    "        avg_training_loss = train_epoch(train_dataloader, model, \n",
    "                                      criterion, optimizer, \n",
    "                                      scheduler, GradScaler(), \n",
    "                                      Config.accum_iter, LOGGER,\n",
    "                                      device, tb_writer, fold, e)\n",
    "\n",
    "        # -------- VALIDATE --------\n",
    "        avg_validation_loss, preds = valid_epoch(valid_dataloader, model, \n",
    "                                                 criterion, LOGGER, device, \n",
    "                                                 tb_writer, fold, e)\n",
    "        \n",
    "        train_losses.append(avg_training_loss)\n",
    "        val_losses.append(avg_validation_loss)\n",
    "\n",
    "        # -------- SCORE METRICS & LOGGING FOR THIS EPOCH --------\n",
    "        validation_labels = valid_df[Config.target_col].values\n",
    "        accuracy = accuracy_score(y_true=validation_labels, y_pred=preds)\n",
    "       \n",
    "        epoch_elapsed_time = time.time() - epoch_start_time\n",
    "        \n",
    "        tb_writer.add_scalar(f'Avg Epoch Train Loss Fold {fold}', avg_training_loss, e)\n",
    "        tb_writer.add_scalar(f'Avg Epoch Val Loss Fold {fold}', avg_validation_loss, e)\n",
    "        tb_writer.add_scalar(f'Epoch Val Accuracy Fold {fold}', accuracy, e)\n",
    "        \n",
    "        LOGGER.info(f'\\nEpoch training summary:\\n Fold {fold+1}/{Config.fold_num} | ' + \\\n",
    "                    f'Epoch: {e+1}/{Config.epochs} | ' + \\\n",
    "                    f'Epoch time: {epoch_elapsed_time} sec\\n' + \\\n",
    "                    f'Training loss: {avg_training_loss} | ' + \\\n",
    "                    f'Validation loss: {avg_validation_loss} | ' + \\\n",
    "                    f'Accuracy: {accuracy}')\n",
    "        \n",
    "        early_stop(avg_validation_loss)\n",
    "        if early_stop.stop: break\n",
    "            \n",
    "        # --------SAVE MODEL --------\n",
    "        if avg_validation_loss < best_val_loss: \n",
    "            best_val_loss = avg_validation_loss\n",
    "            torch.save({'model_state': model.state_dict(), \n",
    "                        'optimizer_state': optimizer.state_dict(),\n",
    "                        'accuracy': accuracy, \n",
    "                        'preds': preds,\n",
    "                        'val_loss': best_val_loss,\n",
    "                        'fold': fold,\n",
    "                        'epochs_no_improve': early_stop.counter,\n",
    "                        'epoch_stopped_at': e\n",
    "                       }, model_checkpoint_name)\n",
    "            LOGGER.info(f'Saved model!')\n",
    "        LOGGER.info('----------------')\n",
    "        \n",
    "        # -------- UPDATE LR --------\n",
    "        if scheduler and e > 2:\n",
    "            if Config.scheduler == 'ReduceLROnPlateau':\n",
    "                scheduler.step(avg_validation_loss)\n",
    "        gc.collect()\n",
    "\n",
    "    # -------- TEST ON HOLDOUT SET --------\n",
    "    # load best model\n",
    "    checkpoint = torch.load(model_checkpoint_name)\n",
    "    model.load_state_dict(checkpoint['model_state']) \n",
    "    holdout_loss, holdout_preds = valid_epoch(holdout_dataloader, model, \n",
    "                                              criterion, LOGGER, device, \n",
    "                                              tb_writer, fold, holdout=True)\n",
    "    holdout_accuracy = accuracy_score(y_true=holdout_targets, y_pred=holdout_preds)\n",
    "    \n",
    "    tb_writer.add_scalar(f'Fold {fold} holdout accuracy', holdout_accuracy, fold)\n",
    "    tb_writer.add_scalar(f'Fold {fold} holdout loss', holdout_loss, fold)\n",
    "    \n",
    "    valid_df['prediction'] = checkpoint['preds']\n",
    "    \n",
    "    del model\n",
    "    del optimizer\n",
    "    del train_dataloader\n",
    "    del valid_dataloader\n",
    "    return valid_df, checkpoint['accuracy'], holdout_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_folds(device, basename, tb_writer, \n",
    "                folds=None, holdout_dataloader=None, holdout_targets=None, checkpoint=None, train_fold=None):\n",
    "    LOGGER.info('========== Running training ==========\\n')\n",
    "    aggregated_output_df = pd.DataFrame()\n",
    "    accuracies = []\n",
    "\n",
    "    starting_fold = 0\n",
    "    if checkpoint: # resume from middle of a fold\n",
    "        starting_fold = checkpoint['fold']\n",
    "    elif train_fold is not None:\n",
    "        starting_fold = train_fold\n",
    "        \n",
    "    folds = pd.read_csv(basename + '/folds.csv', engine='python')\n",
    "    holdout_df = pd.read_csv(basename + '/holdout.csv', engine='python')\n",
    "    holdout_dataloader, holdout_targets = create_holdout_loader(holdout_df, Config.train_img_dir)\n",
    "\n",
    "    time_training_start = time.time()\n",
    "\n",
    "    for fold in range(starting_fold, Config.fold_num):    \n",
    "        # _df is the validation prediction output that we will save to file\n",
    "        # _df.columns: ['image_id', 'label', 'fold', 'prediction']\n",
    "        _df, val_accuracy, holdout_accuracy = train_valid_test(\n",
    "                                                               folds, fold, device,\n",
    "                                                               basename,\n",
    "                                                               holdout_dataloader, \n",
    "                                                               holdout_targets, \n",
    "                                                               tb_writer, checkpoint)\n",
    "\n",
    "        if aggregated_output_df.empty:\n",
    "            aggregated_output_df[['image_id', 'label']] = _df[['image_id', 'label']]\n",
    "        aggregated_output_df[['prediction_fold' + str(fold)]] = _df['prediction']\n",
    "\n",
    "        accuracies.append((val_accuracy, holdout_accuracy))\n",
    "\n",
    "        LOGGER.info(f'========== fold: {fold+1}/{Config.fold_num} result ==========')\n",
    "        LOGGER.info(f'Best Validation Accuracy: {val_accuracy}')\n",
    "        LOGGER.info(f'Holdout Accuracy: {holdout_accuracy}')\n",
    "        LOGGER.info(f'Holdout Loss: {holdout_loss}')\n",
    "\n",
    "    # Cross validation\n",
    "    time_elapsed_training = time.time() - time_training_start \n",
    "    LOGGER.info(f\"Training time: {str(timedelta(seconds=time_elapsed_training))}\")\n",
    "    LOGGER.info(f\"Accuracy (best val, holdout): {accuracies}\")\n",
    "\n",
    "    # Save result\n",
    "    aggregated_output_df.to_csv(basename + f'/aggregated_output_df.csv', index=False)\n",
    "    LOGGER.info('========== Training complete ==========\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Entry point to training.\n",
    "experiment_name_dir (required): a name for a directory in ./trained-models \n",
    "\"\"\"\n",
    "def run_training(experiment_name_dir, resume, train_fold=None):\n",
    "    assert train_fold is None or train_fold in range(Config.fold_num)\n",
    "    basename = Config.save_dir + f'/{experiment_name_dir}'\n",
    "    \n",
    "    try:\n",
    "        # -------- SETUP --------\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        tb_writer = SummaryWriter(f'./runs/{experiment_name_dir}')\n",
    "        \n",
    "        # resume training \n",
    "        if resume and train_fold:\n",
    "            train_folds(device=device, basename=basename, tb_writer=tb_writer, checkpoint=None, train_fold=train_fold)\n",
    "        elif resume and train_fold is None:\n",
    "            last_fold = np.max([int(f[-5]) for f in os.listdir(basename) if f[-3:] == 'pth'])\n",
    "            if last_fold >= 0: \n",
    "                print(f'Experiment exists. Resuming training from latest fold ({last_fold}).')\n",
    "                checkpoint = torch.load(basename + f'/{Config.model_arch}_fold{last_fold}.pth')\n",
    "                train_folds(device=device, basename=basename, tb_writer=tb_writer, checkpoint=checkpoint)\n",
    "        else:\n",
    "            # MAKE DIRECTORY FOR EXPERIMENT MODELS AND FILES\n",
    "            try:\n",
    "                os.makedirs(Config.save_dir + f'/{experiment_name_dir}')\n",
    "            except OSError as e:\n",
    "                print('Experiment already exists and resume flag is not set. Abort training.')\n",
    "                if e.errno != errno.EEXIST: raise\n",
    "            \n",
    "            # -------- LOAD DATA FROM TRAIN FILE --------\n",
    "            data_df, sample_df, holdout_df = read_csvs(Config.data_dir, Config.debug, test_proportion=0.15)\n",
    "            folds = stratify_split(data_df, Config.fold_num, Config.seed, Config.target_col)\n",
    "\n",
    "            \n",
    "            # create holdout dataloader to validate each fold on unseen data\n",
    "            holdout_dataloader, holdout_targets = create_holdout_loader(holdout_df, Config.data_dir + '/train_images')\n",
    "            \n",
    "            # -------- SAVE CONFIG AND HOLDOUT --------\n",
    "            # save folds to file\n",
    "            folds.to_csv(basename + f'/folds.csv', index=False)\n",
    "            # save holdout to a csv file for final inference (so we don't run inference on training examples)\n",
    "            holdout_df.to_csv(basename + f'/holdout.csv', index=False)\n",
    "            # copy the config file for this experiment to this directory\n",
    "            shutil.copy2('./config.py', basename)\n",
    "            train_folds(folds=folds, device=device, basename=basename, tb_writer=tb_writer,\n",
    "                       holdout_dataloader=holdout_dataloader, holdout_targets=holdout_targets)        \n",
    "    finally: \n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes: fold 1 is just model from fold 0 trained more... this is accidental - it used model state from previous fold when loading model. this has been fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        print('Running in debug mode:', Config.debug)\n",
    "        run_training(experiment_name_dir='exp6_sgd', resume=True, train_fold=4)\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}