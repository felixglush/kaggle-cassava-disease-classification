{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cassava Leaf Disease Classification\n",
    "\n",
    "This notebook builds and trains a model for cassava leaf disease classification for the [Kaggle competition](https://www.kaggle.com/c/cassava-leaf-disease-classification/overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas\n",
    "\n",
    "1. Cross entropy loss, stratified CV, no fmix, cutmix, mixup, w gradient scaling & accumulation [done]\n",
    "2. add hyperparam tuning with raytune [done]\n",
    "2. Add smoothed cross entropy loss\n",
    "3. Add *mixes\n",
    "4. external data\n",
    "5. emsemble of models - train a model for each fold and then average their predictions during inference [done]\n",
    "6. train 15-20 epochs [done]\n",
    "7. Test time augmentation\n",
    "8. Better ensemble prediction - majority vote [done], other...?\n",
    "10. train a resnet model\n",
    "11. balanced classes instead of stratified?\n",
    "12. verify per class accuracy\n",
    "13. AdaBound - \"as good as SGD and as fast as Adam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import joblib\n",
    "import warnings\n",
    "import gc\n",
    "import errno\n",
    "import shutil\n",
    "\n",
    "# My modules\n",
    "from config import Config\n",
    "from logger import init_logger\n",
    "from common_utils import (set_seeds, read_csvs, stratify_split, setup_model_optimizer, \n",
    "                          get_data_dfs, get_loaders, create_holdout_loader, get_schd_crit)\n",
    "from model import Model\n",
    "from train_loop_functions import train_epoch, valid_epoch, ensemble_inference\n",
    "from cassava_dataset import CassavaDataset\n",
    "from early_stopping import EarlyStopping\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cassava Bacterial Blight (CBB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cassava Brown Streak Disease (CBSD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cassava Green Mottle (CGM)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cassava Mosaic Disease (CMD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     0\n",
       "0       Cassava Bacterial Blight (CBB)\n",
       "1  Cassava Brown Streak Disease (CBSD)\n",
       "2           Cassava Green Mottle (CGM)\n",
       "3         Cassava Mosaic Disease (CMD)\n",
       "4                              Healthy"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = pd.read_json(Config.data_dir + '/label_num_to_disease_map.json', orient='index')\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(Config.seed)\n",
    "LOGGER = init_logger() # uses Python's logging framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Functions\n",
    "\n",
    "gradient scaling https://pytorch.org/docs/stable/notes/amp_examples.html\n",
    "\n",
    "gradient accumulation https://towardsdatascience.com/what-is-gradient-accumulation-in-deep-learning-ec034122cfa\n",
    "\n",
    "https://towardsdatascience.com/deep-learning-model-training-loop-e41055a24b73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Trains the model over N epochs for a given fold\n",
    "    \n",
    "    train_folds_df: the dataset with a column for fold number\n",
    "    fold: an integer representing the fold used for validation\n",
    "    \n",
    "    Returns a DataFrame consisting of only the the rows used for validation along with the model's predictions\n",
    "''' \n",
    "def train_valid_test(train_folds_df, fold, \n",
    "                     device, basename, \n",
    "                     holdout_dataloader, holdout_targets, \n",
    "                     tb_writer, checkpoint=None):\n",
    "    model_checkpoint_name = basename + f'/{Config.model_arch}_fold{fold}.pth'\n",
    "    \n",
    "    # -------- DATASETS AND LOADERS --------\n",
    "    # select one of the folds, create train & validation set loaders\n",
    "    train_df, valid_df = get_data_dfs(train_folds_df, fold)\n",
    "    train_dataloader, valid_dataloader = get_loaders(train_df, valid_df,\n",
    "                                                     Config.train_bs, \n",
    "                                                     Config.data_dir + '/train_images')\n",
    "    \n",
    "    # make model and optimizer\n",
    "    model, optimizer = setup_model_optimizer(Config.model_arch, \n",
    "                                           Config.lr, \n",
    "                                           Config.is_amsgrad, \n",
    "                                           num_labels=train_folds_df.label.nunique(), \n",
    "                                           weight_decay=Config.weight_decay,\n",
    "                                           momentum=Config.momentum,\n",
    "                                           fc_layer={\"middle_fc\": False, \"middle_fc_size\": 0},\n",
    "                                           device=device,\n",
    "                                           checkpoint=checkpoint)\n",
    "    \n",
    "    scheduler, criterion = get_schd_crit(optimizer)\n",
    "    \n",
    "    accuracy = 0.\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    early_stop = EarlyStopping('val_loss', LOGGER, patience=Config.loss_patience)\n",
    "\n",
    "    for e in range(Config.epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        LOGGER.info(f'Training epoch {e+1}/{Config.epochs}')\n",
    "        \n",
    "        # -------- TRAIN --------\n",
    "        avg_training_loss = train_epoch(train_dataloader, model, \n",
    "                                      criterion, optimizer, \n",
    "                                      scheduler, GradScaler(), \n",
    "                                      Config.accum_iter, LOGGER,\n",
    "                                      device, tb_writer, fold, e)\n",
    "\n",
    "        # -------- VALIDATE --------\n",
    "        avg_validation_loss, preds = valid_epoch(valid_dataloader, model, \n",
    "                                                 criterion, LOGGER, device, \n",
    "                                                 tb_writer, fold, e)\n",
    "        \n",
    "        train_losses.append(avg_training_loss)\n",
    "        val_losses.append(avg_validation_loss)\n",
    "\n",
    "        # -------- SCORE METRICS & LOGGING FOR THIS EPOCH --------\n",
    "        validation_labels = valid_df[Config.target_col].values\n",
    "        accuracy = accuracy_score(y_true=validation_labels, y_pred=preds)\n",
    "       \n",
    "        epoch_elapsed_time = time.time() - epoch_start_time\n",
    "        \n",
    "        tb_writer.add_scalar(f'Avg Epoch Train Loss Fold {fold}', avg_training_loss, e)\n",
    "        tb_writer.add_scalar(f'Avg Epoch Val Loss Fold {fold}', avg_validation_loss, e)\n",
    "        tb_writer.add_scalar(f'Epoch Val Accuracy Fold {fold}', accuracy, e)\n",
    "        \n",
    "        LOGGER.info(f'\\nEpoch training summary:\\n Fold {fold+1}/{Config.fold_num} | ' + \\\n",
    "                    f'Epoch: {e+1}/{Config.epochs} | ' + \\\n",
    "                    f'Epoch time: {epoch_elapsed_time} sec\\n' + \\\n",
    "                    f'Training loss: {avg_training_loss} | ' + \\\n",
    "                    f'Validation loss: {avg_validation_loss} | ' + \\\n",
    "                    f'Accuracy: {accuracy}')\n",
    "        \n",
    "        early_stop(avg_validation_loss)\n",
    "        if early_stop.stop: break\n",
    "            \n",
    "        # --------SAVE MODEL --------\n",
    "        if avg_validation_loss < best_val_loss: \n",
    "            best_val_loss = avg_validation_loss\n",
    "            torch.save({'model_state': model.state_dict(), \n",
    "                        'optimizer_state': optimizer.state_dict(),\n",
    "                        'accuracy': accuracy, \n",
    "                        'preds': preds,\n",
    "                        'val_loss': best_val_loss,\n",
    "                        'fold': fold,\n",
    "                        'epochs_no_improve': early_stop.counter,\n",
    "                        'epoch_stopped_at': e\n",
    "                       }, model_checkpoint_name)\n",
    "            LOGGER.info(f'Saved model!')\n",
    "        LOGGER.info('----------------')\n",
    "        \n",
    "        # -------- UPDATE LR --------\n",
    "        if scheduler and e > 2:\n",
    "            if Config.scheduler == 'ReduceLROnPlateau':\n",
    "                scheduler.step(avg_validation_loss)\n",
    "        gc.collect()\n",
    "\n",
    "    # -------- TEST ON HOLDOUT SET --------\n",
    "    # load best model\n",
    "    checkpoint = torch.load(model_checkpoint_name)\n",
    "    model.load_state_dict(checkpoint['model_state']) \n",
    "    holdout_loss, holdout_preds = valid_epoch(holdout_dataloader, model, \n",
    "                                              criterion, LOGGER, device, \n",
    "                                              tb_writer, fold, holdout=True)\n",
    "    holdout_accuracy = accuracy_score(y_true=holdout_targets, y_pred=holdout_preds)\n",
    "    \n",
    "    tb_writer.add_scalar(f'Fold {fold} holdout accuracy', holdout_accuracy, fold)\n",
    "    tb_writer.add_scalar(f'Fold {fold} holdout loss', holdout_loss, fold)\n",
    "    \n",
    "    valid_df['prediction'] = checkpoint['preds']\n",
    "    \n",
    "    del model\n",
    "    del optimizer\n",
    "    del train_dataloader\n",
    "    del valid_dataloader\n",
    "    return valid_df, checkpoint['accuracy'], holdout_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_folds(device, basename, tb_writer, \n",
    "                folds=None, holdout_dataloader=None, holdout_targets=None, checkpoint=None, train_fold=None):\n",
    "    LOGGER.info('========== Running training ==========\\n')\n",
    "    aggregated_output_df = pd.DataFrame()\n",
    "    accuracies = []\n",
    "\n",
    "    starting_fold = 0\n",
    "    if checkpoint: # resume from middle of a fold\n",
    "        starting_fold = checkpoint['fold']\n",
    "    elif train_fold is not None:\n",
    "        starting_fold = train_fold\n",
    "        \n",
    "    folds = pd.read_csv(basename + '/folds.csv', engine='python')\n",
    "    holdout_df = pd.read_csv(basename + '/holdout.csv', engine='python')\n",
    "    holdout_dataloader, holdout_targets = create_holdout_loader(holdout_df, Config.data_dir + '/train_images')\n",
    "\n",
    "    time_training_start = time.time()\n",
    "\n",
    "    for fold in range(starting_fold, Config.fold_num):    \n",
    "        # _df is the validation prediction output that we will save to file\n",
    "        # _df.columns: ['image_id', 'label', 'fold', 'prediction']\n",
    "        _df, val_accuracy, holdout_accuracy, holdout_loss = train_valid_test(\n",
    "                                                               folds, fold, device,\n",
    "                                                               basename,\n",
    "                                                               holdout_dataloader, \n",
    "                                                               holdout_targets, \n",
    "                                                               tb_writer, checkpoint)\n",
    "\n",
    "        if aggregated_output_df.empty:\n",
    "            aggregated_output_df[['image_id', 'label']] = _df[['image_id', 'label']]\n",
    "        aggregated_output_df[['prediction_fold' + str(fold)]] = _df['prediction']\n",
    "\n",
    "        accuracies.append((val_accuracy, holdout_accuracy))\n",
    "\n",
    "        LOGGER.info(f'========== fold: {fold+1}/{Config.fold_num} result ==========')\n",
    "        LOGGER.info(f'Best Validation Accuracy: {val_accuracy}')\n",
    "        LOGGER.info(f'Holdout Accuracy: {holdout_accuracy}')\n",
    "        Logger.info(f'Holdout Loss: {holdout_loss}')\n",
    "\n",
    "    # Cross validation\n",
    "    time_elapsed_training = time.time() - time_training_start \n",
    "    LOGGER.info(f\"Training time: {str(timedelta(seconds=time_elapsed_training))}\")\n",
    "    LOGGER.info(f\"Accuracy (best val, holdout): {accuracies}\")\n",
    "\n",
    "    # Save result\n",
    "    aggregated_output_df.to_csv(basename + f'/aggregated_output_df.csv', index=False)\n",
    "    LOGGER.info('========== Training complete ==========\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Entry point to training and inference. \n",
    "experiment_name_dir (required): a name for a directory in ./trained-models \n",
    "\"\"\"\n",
    "def run_training(experiment_name_dir, resume, train_fold=None):\n",
    "    assert train_fold is None or train_fold in range(Config.fold_num)\n",
    "    basename = Config.save_dir + f'/{experiment_name_dir}'\n",
    "    \n",
    "    try:\n",
    "        # -------- SETUP --------\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        tb_writer = SummaryWriter(f'./runs/{experiment_name_dir}')\n",
    "        \n",
    "        # resume training \n",
    "        if resume and train_fold:\n",
    "            train_folds(device=device, basename=basename, tb_writer=tb_writer, checkpoint=None, train_fold=train_fold)\n",
    "        elif resume and train_fold is None:\n",
    "            last_fold = np.max([int(f[-5]) for f in os.listdir(basename) if f[-3:] == 'pth'])\n",
    "            if last_fold >= 0: \n",
    "                print(f'Experiment exists. Resuming training from latest fold ({last_fold}).')\n",
    "                checkpoint = torch.load(basename + f'/{Config.model_arch}_fold{last_fold}.pth')\n",
    "                train_folds(device=device, basename=basename, tb_writer=tb_writer, checkpoint=checkpoint)\n",
    "        else:\n",
    "            # MAKE DIRECTORY FOR EXPERIMENT MODELS AND FILES\n",
    "            try:\n",
    "                os.makedirs(Config.save_dir + f'/{experiment_name_dir}')\n",
    "            except OSError as e:\n",
    "                print('Experiment already exists and resume flag is not set. Abort training.')\n",
    "                if e.errno != errno.EEXIST: raise\n",
    "            \n",
    "            # -------- LOAD DATA FROM TRAIN FILE --------\n",
    "            data_df, sample_df, holdout_df = read_csvs(Config.data_dir, Config.debug, test_proportion=0.15)\n",
    "            folds = stratify_split(data_df, Config.fold_num, Config.seed, Config.target_col)\n",
    "\n",
    "            # create holdout dataloader to validate each fold on unseen data\n",
    "            holdout_dataloader, holdout_targets = create_holdout_loader(holdout_df, Config.data_dir + '/train_images')\n",
    "            \n",
    "            # -------- SAVE CONFIG AND HOLDOUT --------\n",
    "            # save folds to file\n",
    "            folds.to_csv(basename + f'/folds.csv', index=False)\n",
    "            # save holdout to a csv file for final inference (so we don't run inference on training examples)\n",
    "            holdout_df.to_csv(basename + f'/holdout.csv', index=False)\n",
    "            # copy the config file for this experiment to this directory\n",
    "            shutil.copy2('./config.py', basename)\n",
    "            train_folds(folds=folds, device=device, basename=basename, tb_writer=tb_writer,\n",
    "                       holdout_dataloader=holdout_dataloader, holdout_targets=holdout_targets)        \n",
    "    finally: \n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes: fold 1 is just model from fold 0 trained more... this is accidental - it used model state from previous fold when loading model. this has been fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== Running training ==========\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in debug mode: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1/25\n",
      "[TRAIN] Processing batch 910: 100%|██████████| 910/910 [08:07<00:00,  1.87it/s]\n",
      "[TRAIN] batch loss: 0.11343315967491695\n",
      "[VAL] Processing batch 114: 100%|██████████| 114/114 [01:17<00:00,  1.46it/s]\n",
      "[VAL] batch loss: 0.5940996638515539\n",
      "\n",
      "Epoch training summary:\n",
      " Fold 3/5 | Epoch: 1/25 | Epoch time: 565.468302488327 sec\n",
      "Training loss: 0.11343315967491695 | Validation loss: 0.5940996638515539 | Accuracy: 0.8036843552378333\n",
      "Saved model!\n",
      "----------------\n",
      "Training epoch 2/25\n",
      "[TRAIN] Processing batch 910: 100%|██████████| 910/910 [07:55<00:00,  1.91it/s]\n",
      "[TRAIN] batch loss: 0.07420954324693961\n",
      "[VAL] Processing batch 114: 100%|██████████| 114/114 [01:16<00:00,  1.48it/s]\n",
      "[VAL] batch loss: 0.4795709317713453\n",
      "\n",
      "Epoch training summary:\n",
      " Fold 3/5 | Epoch: 2/25 | Epoch time: 552.7640726566315 sec\n",
      "Training loss: 0.07420954324693961 | Validation loss: 0.4795709317713453 | Accuracy: 0.8276051690954083\n",
      "Saved model!\n",
      "----------------\n",
      "Training epoch 3/25\n",
      "[TRAIN] Processing batch 910: 100%|██████████| 910/910 [07:56<00:00,  1.91it/s]\n",
      "[TRAIN] batch loss: 0.06761907297954127\n",
      "[VAL] Processing batch 114: 100%|██████████| 114/114 [01:17<00:00,  1.47it/s]\n",
      "[VAL] batch loss: 0.46826471349126414\n",
      "\n",
      "Epoch training summary:\n",
      " Fold 3/5 | Epoch: 3/25 | Epoch time: 553.6862297058105 sec\n",
      "Training loss: 0.06761907297954127 | Validation loss: 0.46826471349126414 | Accuracy: 0.8331042067638164\n",
      "Saved model!\n",
      "----------------\n",
      "Training epoch 4/25\n",
      "[TRAIN] Processing batch 910: 100%|██████████| 910/910 [07:58<00:00,  1.90it/s]\n",
      "[TRAIN] batch loss: 0.06124042595460356\n",
      "[VAL] Processing batch 114: 100%|██████████| 114/114 [01:17<00:00,  1.46it/s]\n",
      "[VAL] batch loss: 0.38097065157796206\n",
      "\n",
      "Epoch training summary:\n",
      " Fold 3/5 | Epoch: 4/25 | Epoch time: 556.423540353775 sec\n",
      "Training loss: 0.06124042595460356 | Validation loss: 0.38097065157796206 | Accuracy: 0.8680230959582074\n",
      "Saved model!\n",
      "----------------\n",
      "Training epoch 5/25\n",
      "[TRAIN] Processing batch 910: 100%|██████████| 910/910 [07:57<00:00,  1.91it/s]\n",
      "[TRAIN] batch loss: 0.05758230194684814\n",
      "[VAL] Processing batch 114: 100%|██████████| 114/114 [01:16<00:00,  1.48it/s]\n",
      "[VAL] batch loss: 0.37446796482330874\n",
      "\n",
      "Epoch training summary:\n",
      " Fold 3/5 | Epoch: 5/25 | Epoch time: 553.9436626434326 sec\n",
      "Training loss: 0.05758230194684814 | Validation loss: 0.37446796482330874 | Accuracy: 0.8674731921913665\n",
      "Saved model!\n",
      "----------------\n",
      "Training epoch 6/25\n",
      "[TRAIN] Processing batch 910: 100%|██████████| 910/910 [07:55<00:00,  1.91it/s]\n",
      "[TRAIN] batch loss: 0.05356658888612311\n",
      "[VAL] Processing batch 114: 100%|██████████| 114/114 [01:16<00:00,  1.48it/s]\n",
      "[VAL] batch loss: 0.4252055635054906\n",
      "\n",
      "Epoch training summary:\n",
      " Fold 3/5 | Epoch: 6/25 | Epoch time: 552.92214179039 sec\n",
      "Training loss: 0.05356658888612311 | Validation loss: 0.4252055635054906 | Accuracy: 0.8487764641187792\n",
      "----------------\n",
      "Training epoch 7/25\n",
      "[TRAIN] Processing batch 910: 100%|██████████| 910/910 [07:55<00:00,  1.91it/s]\n",
      "[TRAIN] batch loss: 0.05931401915981301\n",
      "[VAL] Processing batch 114: 100%|██████████| 114/114 [01:16<00:00,  1.48it/s]\n",
      "[VAL] batch loss: 0.3745936391254266\n",
      "\n",
      "Epoch training summary:\n",
      " Fold 3/5 | Epoch: 7/25 | Epoch time: 552.7069547176361 sec\n",
      "Training loss: 0.05931401915981301 | Validation loss: 0.3745936391254266 | Accuracy: 0.8669232884245257\n",
      "----------------\n",
      "Training epoch 8/25\n",
      "[TRAIN] Processing batch 910: 100%|██████████| 910/910 [07:55<00:00,  1.91it/s]\n",
      "[TRAIN] batch loss: 0.05016686207113358\n",
      "[VAL] Processing batch 108:  94%|█████████▍| 107/114 [01:13<00:04,  1.43it/s]"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        print('Running in debug mode:', Config.debug)\n",
    "        run_training(experiment_name_dir='exp6_sgd', resume=True, train_fold=2)\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
