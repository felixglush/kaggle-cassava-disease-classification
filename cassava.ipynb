{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cassava Leaf Disease Classification\n",
    "\n",
    "This notebook builds and trains a model for cassava leaf disease classification for the [Kaggle competition](https://www.kaggle.com/c/cassava-leaf-disease-classification/overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas\n",
    "\n",
    "1. Cross entropy loss, stratified CV, no fmix, cutmix, mixup, w gradient scaling & accumulation [done]\n",
    "2. add hyperparam tuning with raytune\n",
    "2. Add smoothed cross entropy loss\n",
    "3. Add *mixes\n",
    "4. external data\n",
    "5. emsemble of models - train a model for each fold and then average their predictions during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "package_paths = [\n",
    "    # this is a project by Ross Wightman (https://github.com/rwightman/pytorch-image-models)\n",
    "    '../pytorch-image-models'\n",
    "]\n",
    "import sys; \n",
    "\n",
    "for pth in package_paths:\n",
    "    sys.path.append(pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "from config import Config # my configurations file with hyperparams and constants\n",
    "from logger import init_logger\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from skimage import io\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from sklearn.metrics import roc_auc_score, log_loss, accuracy_score\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import timm # pytorch-image-models implementations\n",
    "\n",
    "from functools import partial\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary data loading and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(Config.data_csv, engine='python')\n",
    "test = pd.read_csv(Config.data_dir + '/sample_submission.csv', engine='python')\n",
    "if Config.debug:\n",
    "    train = train.sample(n=200, random_state=Config.seed).reset_index(drop=True)\n",
    "    Config.epochs = 1\n",
    "    \n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = pd.read_json(Config.data_dir + '/label_num_to_disease_map.json', orient='index')\n",
    "label_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the distribution of classes is uneven, we could do stratified k-fold cross validation to make each fold's train and validation distributions representative of the original distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    \n",
    "def get_image(path):\n",
    "    img_bgr = cv2.imread(path)\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    # im_rgb = im_bgr[:, :, ::-1] would also work -> select all x, y, and reverse the color channels\n",
    "    return img_rgb\n",
    "\n",
    "set_seeds(Config.seed)\n",
    "LOGGER = init_logger() # uses Python's logging framework\n",
    "\n",
    "sample_img = get_image(Config.train_img_dir + '/1000015157.jpg')\n",
    "plt.imshow(sample_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo ideas: add cutmix, fmix, mixup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CassavaDataset(Dataset):\n",
    "    def __init__(self, df, data_root_dir, transform=None, output_label=False):\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        self.data_root_dir = data_root_dir\n",
    "        self.transform = transform\n",
    "        self.output_label = output_label\n",
    "        self.labels = self.df.label.values\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = get_image('{}/{}'.format(self.data_root_dir, self.df.image_id[idx]))\n",
    "        \n",
    "        if self.transform:\n",
    "             img = self.transform(image=img)['image']\n",
    "                \n",
    "        if self.output_label == True:\n",
    "            return img, self.labels[idx]\n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cassava_dataset = CassavaDataset(train, Config.train_img_dir, output_label=True)\n",
    "fig = plt.figure()\n",
    "for i in range(2):\n",
    "    img, target = cassava_dataset[i]\n",
    "    print(i, img.shape, target)\n",
    "    \n",
    "    ax = plt.subplot(1, 2, i + 1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title('Class: {}'.format(target))\n",
    "    ax.axis('off')\n",
    "    plt.imshow(img)\n",
    "    if i == 3:\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Cross Validation Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folds = train.copy()\n",
    "stratifiedFold = StratifiedKFold(n_splits=Config.fold_num, shuffle=True, random_state=Config.seed)\n",
    "splits = stratifiedFold.split(np.zeros(len(train_folds)), train_folds[Config.target_col])\n",
    "\n",
    "# label all rows of train_folds with a particular validation set fold number they are part of \n",
    "# (to select the row for validation when splitting on that fold)\n",
    "for fold_num, (train_idxs, val_idxs) in enumerate(splits):\n",
    "    train_folds.loc[val_idxs, 'fold'] = fold_num\n",
    "\n",
    "train_folds['fold'] = train_folds['fold'].astype(int)\n",
    "train_folds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify distributions\n",
    "train_folds.groupby(['fold', 'label']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Train\\Validation Image Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n",
    ")\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "def get_train_transforms():\n",
    "    image_size = Config.img_size\n",
    "    return Compose([\n",
    "            RandomResizedCrop(image_size, image_size),\n",
    "            Transpose(p=0.5),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            VerticalFlip(p=0.5),\n",
    "            ShiftScaleRotate(p=0.5),\n",
    "            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            CoarseDropout(p=0.5),\n",
    "            Cutout(p=0.5),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)\n",
    "  \n",
    "        \n",
    "def get_valid_transforms():\n",
    "    image_size = Config.img_size\n",
    "    return Compose([\n",
    "            CenterCrop(image_size, image_size, p=1.),\n",
    "            Resize(image_size, image_size),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "EfficientNet noisy student: https://arxiv.org/pdf/1911.04252.pdf. Implementation from\n",
    "https://github.com/rwightman/pytorch-image-models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, model_arch,n_classes, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_arch, pretrained=pretrained)\n",
    "        # replace classifier with a Linear in_features->n_classes layer\n",
    "        in_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Linear(in_features, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function \n",
    "\n",
    "Ideas:\n",
    "- https://ai.googleblog.com/2019/08/bi-tempered-logistic-loss-for-training.html\n",
    "- https://github.com/mlpanda/bi-tempered-loss-pytorch/blob/master/bi_tempered_loss.py\n",
    "- https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/173733"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/173733\n",
    "class SmoothedCrossEntropyLoss(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean'):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        lsm = F.log_softmax(inputs, -1)\n",
    "\n",
    "        if self.weight is not None:\n",
    "            lsm = lsm * self.weight.unsqueeze(0)\n",
    "\n",
    "        loss = -(targets * lsm).sum(-1)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiTemperedLoss():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Functions\n",
    "\n",
    "gradient scaling https://pytorch.org/docs/stable/notes/amp_examples.html\n",
    "\n",
    "gradient accumulation https://towardsdatascience.com/what-is-gradient-accumulation-in-deep-learning-ec034122cfa\n",
    "\n",
    "https://towardsdatascience.com/deep-learning-model-training-loop-e41055a24b73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each sample in this batch, take the maximum predicted class\n",
    "def process_model_output(predictions, output, batch_size):\n",
    "    predicted_class_per_sample = np.array([torch.argmax(output, 1).detach().cpu().numpy()])\n",
    "    assert predicted_class_per_sample.shape == (1, batch_size) \n",
    "    predictions = np.concatenate((predictions, predicted_class_per_sample), axis=None)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loops over data with gradient scaling and accumulation\n",
    "def train_epoch(dataloader, model, criterion, optimizer, scheduler, scaler):\n",
    "    model.train()\n",
    "    batch_losses = []\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        with autocast():\n",
    "            predictions = model(images)\n",
    "            loss = criterion(predictions, labels)\n",
    "        \n",
    "        batch_losses.append(loss.item())\n",
    "\n",
    "        # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
    "        # Backward passes under autocast are not recommended.\n",
    "        # Backward ops run in the same dtype autocast chose for corresponding forward ops.\n",
    "        # See https://pytorch.org/docs/stable/amp.html#gradient-scaling for why scaling is helpful\n",
    "        scaler.scale(loss).backward()\n",
    "        total_norm = 0.\n",
    "        \n",
    "        if batch_idx + 1 % Config.accum_iter == 0 or batch_idx + 1 == len(dataloader):\n",
    "            # if want to implement gradient clipping, see this first. need to unscale gradients first.\n",
    "            # https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients\n",
    "        \n",
    "            # Unscales the gradients of optimizer's assigned params in-place\n",
    "            scaler.unscale_(optimizer)\n",
    "\n",
    "            # TEMPORARY\n",
    "            # get gradients to check for explosions and determine clipping value\n",
    "            for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
    "                param_norm = p.grad.data.norm(2).item() # norm of the gradient tensor\n",
    "                total_norm += param_norm ** 2\n",
    "            total_norm = np.sqrt(total_norm)\n",
    "\n",
    "            # Since the gradients of optimizer's assigned params are unscaled, clips as usual:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), Config.max_norm_grad)\n",
    "        \n",
    "            # scaler.step() first unscales (if they're not already) the gradients of the optimizer's assigned params.\n",
    "            # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n",
    "            # otherwise, optimizer.step() is skipped.\n",
    "            scaler.step(optimizer)\n",
    "\n",
    "            # Updates the scale for next iteration.\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "        if batch_idx + 1 % Config.print_every == 0 or batch_idx + 1 == len(dataloader):\n",
    "            LOGGER.info(f'[TRAIN] batch {batch_idx+1}/{len(dataloader)} loss: {loss} | grad: {total_norm}')\n",
    "\n",
    "    return batch_losses\n",
    "\n",
    "def valid_epoch(dataloader, model, criterion):\n",
    "    model.eval()\n",
    "    batch_losses = []\n",
    "    predictions = np.array([])\n",
    "    for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(images)\n",
    "            # output: [batch_size, # classes] -> [batch_size, 5]\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        batch_losses.append(loss)\n",
    "        # for each sample in this batch, take the maximum predicted class\n",
    "        predictions = process_model_output(predictions, output, batch_size=images.size(0))\n",
    "        \n",
    "        if batch_idx + 1 % Config.print_every == 0 or batch_idx + 1 == len(dataloader):\n",
    "            LOGGER.info(f'[VAL] batch {batch_idx+1}/{len(dataloader)} loss: {loss / Config.accum_iter}')\n",
    "        \n",
    "    return batch_losses, predictions\n",
    "    \n",
    "def inference(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = np.array([])\n",
    "    #targets = np.array([])\n",
    "    for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(images)\n",
    "        \n",
    "        # for each sample in this batch, take the maximum predicted class\n",
    "        predictions = process_model_output(predictions, output, batch_size=images.size(0))\n",
    "        \n",
    "        #targets  = np.concatenate((targets, labels), axis=None)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "'''\n",
    "    Trains the model over epochs for a given fold\n",
    "    \n",
    "    train_folds_df: the dataset with a column for fold number\n",
    "    fold: an integer representing the fold used for validation\n",
    "    \n",
    "    Returns a DataFrame consisting of only the the rows used for validation along with the model's predictions\n",
    "''' \n",
    "def train_fold(train_folds_df, fold, model, optimizer, scheduler, criterion, resultsStore):\n",
    "    # -------- DATASETS AND LOADERS --------\n",
    "    train_idx = train_folds_df[train_folds_df['fold'] != fold].index  \n",
    "    valid_idx = train_folds_df[train_folds_df['fold'] == fold].index \n",
    "    train_df = train_folds_df.iloc[train_idx].reset_index(drop=True) # since we are selecting rows, the index will be missing #s so reset\n",
    "    valid_df = train_folds_df.iloc[valid_idx].reset_index(drop=True)\n",
    "    \n",
    "    train_dataset = CassavaDataset(train_df, Config.train_img_dir, output_label=True, transform=get_train_transforms())\n",
    "    valid_dataset = CassavaDataset(valid_df, Config.train_img_dir, output_label=True, transform=get_valid_transforms())\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=Config.train_bs, \n",
    "                                  pin_memory=True, shuffle=True, \n",
    "                                  num_workers=Config.num_workers)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=Config.valid_bs, \n",
    "                                  pin_memory=True, shuffle=True, \n",
    "                                  num_workers=Config.num_workers)\n",
    "    \n",
    "    \n",
    "    accuracy, best_accuracy = 0., 0.\n",
    "    for e in range(Config.epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        LOGGER.info(f'Training epoch {e+1}/{Config.epochs}')\n",
    "        \n",
    "        # -------- TRAIN --------\n",
    "        training_losses = train_epoch(train_dataloader, model, criterion, optimizer, scheduler, GradScaler())\n",
    "        avg_training_loss = sum(training_losses) / len(train_dataloader)\n",
    "        \n",
    "        # -------- VALIDATE --------\n",
    "        validation_losses, preds = valid_epoch(valid_dataloader, model, criterion)\n",
    "        avg_validation_loss = sum(validation_losses) / len(valid_dataloader)\n",
    "        \n",
    "        epoch_elapsed_time = time.time() - epoch_start_time\n",
    "\n",
    "        # -------- SCORE METRICS & LOGGING FOR THIS EPOCH --------\n",
    "        validation_labels = valid_df[Config.target_col].values\n",
    "        accuracy = accuracy_score(y_true=validation_labels, y_pred=preds)\n",
    "        \n",
    "        LOGGER.info(f'\\nEpoch training summary:\\n Fold {fold}/{Config.fold_num} | ' + \\\n",
    "                    f'Epoch: {e+1}/{Config.epochs} | ' + \\\n",
    "                    f'Epoch time: {epoch_elapsed_time} sec | ' + \\\n",
    "                    f'Training loss: {avg_training_loss} | ' + \\\n",
    "                    f'Validation loss: {avg_validation_loss} | ' + \\\n",
    "                    f'Accuracy: {accuracy}\\n')\n",
    "        \n",
    "        # SAVE MODEL (keeps only the best model for this fold)\n",
    "        if accuracy > best_accuracy: \n",
    "            best_accuracy = accuracy\n",
    "            torch.save({'model': model.state_dict(), 'preds': preds, 'accuracy': best_accuracy, 'fold': fold},\n",
    "                      Config.save_dir + f'/{Config.model_arch}_fold{fold}.pth')\n",
    "            LOGGER.info(f'Saved model on epoch {e+1}, fold {fold}, and accuracy score {accuracy:.3f}')\n",
    "        \n",
    "        # -------- UPDATE LR (POTENTIALLY) --------\n",
    "        if scheduler:\n",
    "            if Config.scheduler == 'ReduceLROnPlateau':\n",
    "                scheduler.step(avg_validation_loss)\n",
    "            elif Config.scheduler == 'CosineAnnealingLR' or Config.scheduler == 'CosineAnnealingWarmRestarts':\n",
    "                scheduler.step()\n",
    "\n",
    "    checkpoint = torch.load(Config.save_dir + f'/{Config.model_arch}_fold{fold}.pth')\n",
    "    valid_df['prediction'] = checkpoint['preds']\n",
    "    return valid_df, best_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "class Results:\n",
    "    def __init__(self):\n",
    "        self.fold_to_predictions = {}\n",
    "        self.fold_to_accuracy = {}\n",
    "        \n",
    "def setup():\n",
    "    # -------- SCORES --------\n",
    "    resultsStore = Results()\n",
    "\n",
    "     # -------- MODEL INSTANTIATION --------\n",
    "    model = Model(Config.model_arch, train.label.nunique(), pretrained=True)\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda:0'\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "    # -------- OPTIMIZER --------\n",
    "    optimizer = Adam(model.parameters(), Config.lr, weight_decay=Config.weight_decay, amsgrad=Config.is_amsgrad) # try amsgrad?\n",
    "\n",
    "    # -------- SCHEDULER --------\n",
    "    scheduler = None\n",
    "    if Config.scheduler == 'ReduceLROnPlateau':\n",
    "        scheduler = ReduceLROnPlateau(optimizer, factor=Config.factor, patience=Config.patience, eps=Config.eps, verbose=True)\n",
    "    elif Config.scheduler == 'CosineAnnealingLR':\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=Config.T_max, eta_min=Config.min_lr, verbose=True)\n",
    "    elif Config.scheduler == 'CosineAnnealingWarmRestarts':\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=Config.T_0, T_mult=Config.T_mult, eta_min=Config.min_lr, verbose=True)\n",
    "    \n",
    "    # -------- LOSS FUNCTION --------\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    return resultsStore, model, optimizer, scheduler, criterion\n",
    "        \n",
    "def main(checkpoint_dir=None):    \n",
    "    try:\n",
    "        resultsStore, model, optimizer, scheduler, criterion = setup()\n",
    "            \n",
    "        if Config.train:\n",
    "            LOGGER.info('\\n========== Running training ==========\\n')\n",
    "            \n",
    "            aggregated_output_df = pd.DataFrame()\n",
    "            \n",
    "            for fold in range(Config.fold_num):\n",
    "                # _df is the validation prediction output\n",
    "                # _df.columns: ['image_id', 'label', 'fold', 'prediction']\n",
    "                _df, best_fold_accuracy = train_fold(train_folds, fold, model, optimizer, scheduler, criterion, resultsStore)\n",
    "                \n",
    "                if aggregated_output_df.empty:\n",
    "                    aggregated_output_df[['image_id', 'label']] = _df[['image_id', 'label']]\n",
    "                aggregated_output_df[['prediction_fold'+str(fold)]] = _df['prediction']\n",
    "                \n",
    "                resultsStore.fold_to_predictions[fold] = _df[['image_id', 'label', 'prediction']]\n",
    "                resultsStore.fold_to_accuracy[fold] = best_fold_accuracy\n",
    "                \n",
    "                LOGGER.info(f'========== fold: {fold} result ==========')\n",
    "                LOGGER.info(f'Accuracy: {best_fold_accuracy}')\n",
    "                \n",
    "            # Cross validation\n",
    "            LOGGER.info(f\"========== CV ==========\") # best results across all folds\n",
    "            LOGGER.info(f\"{resultsStore.fold_to_accuracy}\")\n",
    "            \n",
    "            # Save result\n",
    "            aggregated_output_df.to_csv(Config.save_dir + '/aggregated_output_df.csv', index=False)\n",
    "            \n",
    "        if Config.inference: \n",
    "            LOGGER.info('\\n========== Running inference ==========\\n')\n",
    "            test_dataset = CassavaDataset(test, Config.test_img_dir, output_label=True, \n",
    "                                          transform=get_valid_transforms())\n",
    "            \n",
    "            test_dataloader = DataLoader(test_dataset, batch_size=Config.valid_bs, \n",
    "                                  pin_memory=True, shuffle=False, \n",
    "                                  num_workers=Config.num_workers)\n",
    "\n",
    "            predictions = inference(model, test_dataloader)\n",
    "            targets = test.label.values\n",
    "            \n",
    "            # submission\n",
    "            submission = pd.DataFrame()\n",
    "            submission['image_id'] = test['image_id']\n",
    "            submission['label'] = predictions\n",
    "            submission.to_csv(Config.save_dir + '/submission.csv', index=False)\n",
    "    finally: \n",
    "        del model\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each fold, run tune? Each fold gets num_samples trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        print('Training in debug mode: ', Config.debug)\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
