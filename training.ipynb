{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import errno\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from types import SimpleNamespace\n",
    "import torch\n",
    "import warnings\n",
    "from lightning_objects import LightningModel\n",
    "warnings.filterwarnings('ignore')\n",
    "from config import Configuration\n",
    "import pandas as pd\n",
    "from common_utils import stratify_split, make_holdout_df, set_seeds\n",
    "from train_manager import TrainManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def main(experiment_name: str, debug, resume=False,\n",
    "         finetune=False, freeze_bn=True, freeze_feature_extractor=False):\n",
    "\n",
    "    experiment_dir = os.path.abspath(f'trained-models/{experiment_name}')\n",
    "    print('Experiment directory', experiment_dir)\n",
    "\n",
    "    try:\n",
    "        # -------- SETUP --------\n",
    "        checkpoint_params = None\n",
    "        finetune_model_fnames = None\n",
    "        folds_df, holdout_df = None, None\n",
    "\n",
    "        if not resume and not finetune: # totally new experiment\n",
    "            make_experiment_directory(experiment_dir)\n",
    "            config = Configuration()\n",
    "            config.debug = debug\n",
    "            set_seeds(config.seed)\n",
    "\n",
    "            # -------- LOAD DATA FROM TRAIN FILE --------\n",
    "            data_df = pd.read_csv(config.data_dir + '/train.csv', engine='python')\n",
    "            data_df, holdout_df = make_holdout_df(data_df, seed=config.seed)\n",
    "            folds_df = stratify_split(data_df, config.fold_num, config.seed, config.target_col)\n",
    "\n",
    "            # -------- SAVE FILES (for experiment state) --------\n",
    "            folds_df.to_csv(experiment_dir + '/folds.csv', index=False)\n",
    "            # save holdout to a csv file for final inference (so we don't run inference on training examples)\n",
    "            holdout_df.to_csv(experiment_dir + '/holdout.csv', index=False)\n",
    "            with open(experiment_dir + '/experiment_config.json', 'w') as f:\n",
    "                json.dump(config.__dict__, f)\n",
    "        elif resume or finetune:\n",
    "\n",
    "            # LOAD DATA FROM SAVED FILES\n",
    "            with open(experiment_dir + '/experiment_config.json', 'r') as f:\n",
    "                config = json.load(f, object_hook=lambda d: SimpleNamespace(**d))\n",
    "                set_seeds(config.seed)\n",
    "                config.debug = debug\n",
    "\n",
    "            folds_df = pd.read_csv(experiment_dir + '/folds.csv', engine='python')\n",
    "            holdout_df = pd.read_csv(experiment_dir + '/holdout.csv', engine='python')\n",
    "\n",
    "\n",
    "            if finetune and not resume:\n",
    "                print('finetuning...')\n",
    "                # verify there are checkpoints to fine tune\n",
    "                finetune_model_fnames = glob.glob(experiment_dir + '/*fold*.ckpt')\n",
    "                assert len(finetune_model_fnames) > 0\n",
    "                finetune_model_fnames.sort()\n",
    "\n",
    "                # make new directory for tuning experiment with files from training run 1\n",
    "                make_experiment_directory(experiment_dir + '_tune')\n",
    "                for f in os.listdir(experiment_dir):\n",
    "                    print(f\"copying {f} to {experiment_dir + '_tune'}\")\n",
    "                    shutil.copy2(experiment_dir + '/' + f, experiment_dir + '_tune')\n",
    "                experiment_dir += '_tune'\n",
    "                experiment_name += '_tune'\n",
    "            else:\n",
    "                print('resuming from last checkpoint...')\n",
    "                checkpoint_params = get_checkpoint_params(experiment_dir, resume, config.model_arch)\n",
    "\n",
    "        assert holdout_df is not None, 'holdout_df is None'\n",
    "        assert folds_df is not None, 'folds_df is None'\n",
    "\n",
    "        trainer = TrainManager(experiment_name=experiment_name, experiment_dir=experiment_dir,\n",
    "                               folds_df=folds_df, holdout_df=holdout_df,\n",
    "                               checkpoint_params=checkpoint_params, config=config,\n",
    "                               finetune=finetune, freeze_bn=freeze_bn,\n",
    "                               freeze_feature_extractor=freeze_feature_extractor,\n",
    "                               finetune_model_fnames=finetune_model_fnames)\n",
    "        trainer.run()\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def make_experiment_directory(name):\n",
    "    try:\n",
    "        os.makedirs(name)\n",
    "    except FileExistsError as e:\n",
    "        print('Experiment already exists. Be sure to resume training appropriately or start a new experiment.')\n",
    "        if e.errno == errno.EEXIST: raise\n",
    "\n",
    "\n",
    "def get_checkpoint_params(basename, resume, model_arch):\n",
    "    \"\"\"\n",
    "    We can restart from the middle of a fold or start from the beginning of a fold.\n",
    "\n",
    "    checkpoint_params: {\"restart_from\": fold, \"start_beginning_of\": fold, \"checkpoint_file_path\": file}\n",
    "        restart_from (int): start from middle of a fold - typically used when a training session was cancelled mid fold\n",
    "            checkpoint_file_path (str) is required in this case\n",
    "        start_beginning_of (int): train a particular fold\n",
    "    \"\"\"\n",
    "\n",
    "    checkpoint_params = None\n",
    "    if resume:\n",
    "        checkpoint_params = {}\n",
    "        model_filenames = glob.glob(basename + '/*fold*.ckpt')\n",
    "        trained_folds = [re.findall(r'fold\\d+', f)[0][len('fold'):] for f in model_filenames]\n",
    "        most_recent_fold = int(max(trained_folds)) if len(trained_folds) > 0 else 0\n",
    "\n",
    "        checkpoint_params['restart_from'] = most_recent_fold\n",
    "        #checkpoint_params['checkpoint_file_path'] = f'{basename}/{model_arch}_fold{most_recent_fold}.pth'\n",
    "        checkpoint_params['checkpoint_file_path'] = f'{basename}/{model_arch}_fold{1}.pth'\n",
    "\n",
    "    return checkpoint_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        debug = False\n",
    "        print('Running in debug mode:', debug)\n",
    "        main(experiment_name='sgd_only_classifier_tuned_79-25_tune', debug=debug,\n",
    "             resume=True, finetune=False, freeze_bn=True, freeze_feature_extractor=False)\n",
    "        #main(experiment_name='sgd_only_classifier_tuned2', debug=debug,\n",
    "        #     resume=False, finetune=False, freeze_bn=True, freeze_feature_extractor=True)\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
